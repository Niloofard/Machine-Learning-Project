# ðŸ§  Hybrid Mambaâ€“Transformer Architecture for Breast Cancer Classification

### Authors
**Niloofar Dehghani**, **Omid Nejatimanzari**, **Mohammad Naserisafavi**, **Seyed MohammadHosein Enjavimadar**, **Sajad Ahmadian Fini**  
*Applied Machine Learning Course Project â€“ Concordia University (2025)*

---

## ðŸ“˜ Overview
This project explores a **hybrid deep learning architecture** that combines the strengths of **Mamba state-space models** and **Vision Transformers (ViT)** for accurate and efficient **breast cancer image classification**.  
The hybrid model leverages the **long-range dependency modeling** capability of Mamba and the **global attention mechanism** of Transformers to improve diagnostic accuracy while maintaining computational efficiency.

---

## ðŸŽ¯ Objectives
- Develop a **robust hybrid model** that integrates Mamba and Transformer layers.  
- Evaluate the architecture on **breast cancer imaging datasets** (e.g., BreakHis, IDC, or custom histopathology datasets).  
- Compare performance against traditional baselines such as **CNNs, ResNet, ConvNeXt, ViT, and pure Mamba models**.  
- Analyze **accuracy, F1-score, precision, recall**, and **model complexity**.

---

## ðŸ§© Model Architecture
The proposed **Hybrid Mambaâ€“Transformer** combines:
- **Mamba Blocks** for efficient sequence modeling and implicit recurrence.
- **Transformer Blocks** for global context learning.
- A **fusion head** that aggregates both representations before final classification.

The architecture aims to balance **local feature extraction** and **global representation learning**â€”making it highly suitable for medical image analysis tasks.

---

##ðŸ§  Dataset:

## ðŸ“‚ Repository Structure
